---
title: "Field Significance of Regression Patterns"
author: "Elio"
output: 
   pdf_document
urlcolor: blue
bibliography: "field-signf.bib"
---

```{r setup, include=FALSE}
# Notification
start.time <- unclass(Sys.time())
min.time <- 10
knit_doc <- knitr::knit_hooks$get("document")
knitr::knit_hooks$set(document = function(x) {
   took <- unclass(Sys.time()) - start.time
   if (unclass(Sys.time()) - start.time >= min.time) {
      notify("Done knitting!", 
             paste0("Took ", round(took), " seconds"),
             time = 5)
   }  
   knit_doc(x)
})

name <- tools::file_path_sans_ext(knitr::current_input())
knitr::opts_chunk$set(echo = FALSE,
                      cache = TRUE,
                      cache.extra = 1, 
                      warning = FALSE, message = FALSE,
                      out.extra = "", 
                      cache.path = paste0("cache/", name, "/"),
                      fig.path = paste0("fig/", name, "/"))

library(metR)
library(data.table)
library(ggplot2)
library(metR)
library(magrittr)
library(circular)
library(RcppRoll)
library(patchwork)
library(lubridate)

here <- here::here
source("scripts/helperfun.R")

data.world <- BuildMap(res = 1, smooth = 1)
map.world <- geom_map2(data.world)
map.SH <- geom_map2(data.world[lat %b% c(-90, 20)], color = "gray20")

lev.breaks <- c(1000, 500, 300, 200, 100, 50, 10)

theme_elio <- theme_minimal(base_size = 11) +
   theme(
      # text = element_text(family = font_rc),
      legend.position = "bottom", legend.box = "vertical",
      panel.spacing.y = unit(5, "mm"),
      panel.spacing.x = unit(5, "mm"),
      legend.spacing = unit(2, "mm"),
      plot.margin = grid::unit(rep(3, 4), "mm"),
      legend.title = element_blank(),
      legend.box.spacing = unit(3, "mm"),
      legend.margin = margin(t = -5),
      panel.grid = element_line(color = "gray50", size = 0.2, linetype = 3),
      panel.ontop = TRUE)
theme_set(theme_elio)
guide_colorstrip_bottom <- function(width = 25, height = 0.5, ...) {
   guide_colorstrip(title.position = "top", title.hjust = 0.5,
                    barheight = height,
                    barwidth = width, ...)
}
```

Esto es un intento de implementar y entender el método propuesto por [@DelSole2011] para calcular la significancia de los patrones de regresión. 

## Idea básica

La idea es que dado una serie de campos bidimensionales a lo largo del tiempo uno puede armar un campo de regresiones a partir de una serie temporal. Si el campo tiene M celdas, entonces hay que calcular M regresiones:


$$
y(t) = \beta_1x_1(t) + \epsilon_1(t) \\
y(t) = \beta_2x_2(t) + \epsilon_2(t) \\
\cdots  \\
y(t) = \beta_Mx_M(t) + \epsilon_M(t) \\
$$


Donde $x_i(t)$ representa el valor de la variable espacial en la celda iésima y el campo de regresión son los coeficientes $\beta_1, \beta_2, ...\beta_M$. Testear la significancia de este campo no es trivial. En general lo que se hace es testear cada $\beta_i$ por separado como si fuera independiente y obtener un "campo de significancia". Pero eso en realidad no es del todo válido porque ignora la correlación espacial entre los coeficientes y, más importante aún, la multiplicidad de tests. 

[@DelSole2011] propone cambiar el problema. En vez de hacer M regresiones simples, hacer una regresión múltiple: 

$$
y(t) = \beta_1x_1(t) + \beta_2x_2(t) + ... + \beta_Mx_M(t) + \epsilon(t)
$$

para la cual se puede testear la $H_0 := B_i = 0 \; \forall i= 1, 2,...M$ sin más dificultad. Esto elimina el problema de multiplicidad y la correlación espacial. Simple, ¿no? Listo..

No tan rápido. 

## Selecciónd el modelo

Esta regresión funciona si la cantidad de predictores ($M$) es menor que la cantidad de datos usados para realizar el ajuste ($N$), pero en la mayoría de los casos hay mas más celdas que datos. Por ejemplo, un campo global de 2.5° de resolución se tiene $144\times 72 = 10368$ celdas. Si queremos hacer un mapa de regresión con la ecuación anterior usando datos mensuales serían necesarios más de 864 años de datos. Imposible. 

El truquito ahora está en reconocer que la correlación espacial implica que en un campo global no hay $10368$ predictores independientes; existe mucha redundancia de información. Una forma de aprovechar esto es hacer la regresión en el espacio de las componentes principales usando sólo algunas (y siempre menos que $N$) y luego reconstruir el campo conseguido. Si la matriz de datos es $X$ (donde cada columna es un $x_i(t)$), hacemos 

$$
X = UDV^t
$$

Y pasamos a hacer la regresión

$$
y(t) = \beta_1u_1(t) + \beta_2u_2(t) + ... + \beta_Ku_K(t) + \epsilon(t)
$$

Donde $u_i$ son las columnas de $U$. Con $K < N$ nos aseguramos que la regresión ande bien. Luego, el campo de regresión es 
$$
BV^t
$$

(hay algunas constantes de normalización dando vuelta que no son demasiado importantes a la teoría --pero sí a la práctica!)

Simple, ¿no? Listo..

No tan rápido. 

¿Cuántas y cuáles componentes principales elegir? Es el problema eterno. El paper propone seleccionar las primeras K componentes principales usando validación cruzada para juzgar el "mejor K". El procedimiento es el siguiente:

Para k = 1 se ajusta el modelo N veces dejando de lado una observación por vez y computando la diferencia entre el y observado y el modelado. Se consiguen N errores y de estos se computa el $MSE$ y se asume que éste tiene un desvío estándar de $MSE/\sqrt{N}$. Se repíte el procedimiento para todos los k. El resutlado es una serie de $MSE(k)$. Se elije el mayor $k$ para el cual el $MSE$ esté dentro del intervalo de 1 desvío estándar del menor $MSE$ observado. 

De forma general, esto es un problema de *feature selection* que tiene otras soluciones. Una alternativa es usar [LASSO](https://en.wikipedia.org/wiki/Lasso_(statistics)) o [ridge regression](https://en.wikipedia.org/wiki/Tikhonov_regularization), que penaliza los coeficientes "grandes". 

## Ejemplos

### Viento zonal 

Voy a usar datos de viento zonal medio de DEF en 300hPa en el hemisferio norte para replicar lo que hicieron los autores del paper. 

```{r}
library(ggplot2)
library(metR)
library(data.table)
library(magrittr)

world <- subset(map_data("world2"), lat > 0)

geom_world <- geom_path(data = world, aes(long, lat, group = group),
                        size = 0.2)

data <- ReadNetCDF("~/DATOS/NCEP Reanalysis/uwnd.mon.mean.nc", c(u = "uwnd"),
                   subset = list(level = 300,
                                 lat = c(0:90),
                                 time = c("1948-01-01", "2009-12-31"))) %>% 
   .[month(time) %in% c(12, 1, 2)] %>% 
   .[, .(u = mean(u)), by = .(lat, lon, year(time))] 
```

```{r}
source("Regression2D.R")
```

> Una aclaración importante es que los autores del paper usan leave-one-out crossvalidation, pero eso es **eterno** para una cantidad de datos medianamente grande, así que yo implementé k-fold crossvalidation con k = 10 por default. 

```{r}
point_regression <- data[, FitLm(u, year, se = TRUE), by = .(lon, lat)]
cv_regression <- Regression2D(u ~ year | lat + lon, year, data = data) 
lasso_regression <- Regression2D(u ~ year | lat + lon, year, data = data,
                                 method = "lasso") 

regression <- rbindlist(list(point = point_regression[term == "year",
                                                      .(lon, lat, u = estimate)],
                             cv = cv_regression$field, 
                             lasso = lasso_regression$field), 
                        use.names = TRUE, idcol = "type")
```


```{r, fig.height=5}
ggplot(regression, aes(lon, lat)) +
   # geom_raster(aes(fill = u)) +
   geom_contour_fill(aes(z = u),breaks = AnchorBreaks(0, exclude = 0)) +
   geom_world +
   scale_fill_divergent("Trend in U(300hpa)") +
   coord_quickmap() +
   metR:::theme_field() +
   facet_wrap(~type, ncol = 2)
```

Los campos de regresión son prácticamente iguales. La naturaleza de LASSO, que penaliza coeficientes grandes, hace que los valores absolutos sean menores. Podemos ver cuándos EOFs se usaron en la regresión, el $r^2$ del fit y su p-valor siguiendo la metodología del paper

```{r}
rbind(cv = cv_regression$summary,
      lasso = lasso_regression$summary) 
```

Aunque lo patrones son similares, usan distinta cantidad de EOF. La estimación del p-valor no sé si es válida para LASSO, ya que no es cuadrados mínimos, así que la distribución del $r^2$ posiblemente no sea la que dicen en el paper. Pero para ser justos, tampoco sé si con la validación cruzada sigue valiendo. 

Un detalle para el caso de LASSO es que los EOFs que usa varían muchísimo si cambio la cantidad de EOFs que permito entrar a la regresión desde un principio

```{r}
table <- as.data.table(expand.grid(max_eof = c(round(seq(3, 60, length.out = 7)), 61:63),
                                   type = c("lasso", "cv", "neof"))) %>% 
   .[, Regression2D(u ~ year | lat + lon, year, data = data,
                    max_eof = max_eof,
                    method = type)$summary,
     by = .(type, max_eof)] 

lasso_v_cv <- function(value, data = table) {
   dcast(data[type != "neof"], max_eof ~ type, value.var = value)
}

lasso_v_cv(c("non_zero", "r2"))[, lapply(.SD, round, 2)]
```

En particular, se ve que la cantidad de coeficientes no nulos aumenta a medida que se ponen más eofs pero luego disminuye mucho cuando se ponen cerca del máximo. La crossvalidación, en cambio, se queda estable y no es sensible a ese problema (al menos para este ejemplo!)

El $r^2$ también tiene un comportamietno similar. Comparando el $r^2$ en función de la cantidad de coeficientes no nulos para el método LASSO, crossvalidación y el "ingénuo" se ve que el método de crossvalidación deja de incluir EOFs en el "codo" de la curva, mientras que LASSO los sigue incluyendo. 

```{r}
ggplot(table, aes(non_zero, r2)) + 
   geom_point() +
   facet_wrap(~type, ncol = 1)
```

¿Por qué luego baja cuando se incluyen todos los EOFs? No sé. Es posible que tenga que ver con el algoritmo de LASSO, que se comporta diferente según la cantidad de variables en la regresión. Misterio. 

### Viento meridional en  1000hPa

Para probar con otros datos.

```{r}
datav <- ReadNetCDF("~/DATOS/NCEP Reanalysis/vwnd.mon.mean.nc", c(v = "vwnd"),
                   subset = list(level = 1000,
                                 lat = c(0:90),
                                 time = c("1948-01-01", "2009-12-31"))) %>% 
   .[month(time) %in% c(12, 1, 2)] %>% 
   .[, .(v = mean(v)), by = .(lat, lon, year(time))] 
```


```{r}
point_regressionv <- datav[, FitLm(v, year, se = TRUE), by = .(lon, lat)]
cv_regressionv <- Regression2D(v ~ year | lat + lon, year, data = datav) 
lasso_regressionv <- Regression2D(v ~ year | lat + lon, year, data = datav,
                                 method = "lasso") 

regression <- rbindlist(list(point = point_regressionv[term == "year",
                                                      .(lon, lat, v = estimate)],
                             cv = cv_regressionv$field, 
                             lasso = lasso_regressionv$field), 
                        use.names = TRUE, idcol = "type")
```

```{r}
ggplot(regression, aes(lon, lat)) +
   geom_contour_fill(aes(z = v),breaks = AnchorBreaks(0, exclude = 0)) +
   geom_world +
   scale_fill_divergent("Trend in V(1000hpa)") +
   coord_quickmap() +
   metR:::theme_field() +
   facet_wrap(~type, ncol = 2)
```

En estos datos, ambos métodos dan buenos resultados caracterizando el campo de regresión. LASSO tiene los mismos problemas que en el caso anterior. 


### Dependencia con dominio

Un problema de los EOFs es que pueden depender mucho del dominio. Por lo tanto, es esperable que los mapas de regresión también dependan del dominio si se usan EOFs para generarlos. Por ejemplo, estos son los mapas de regresión para U en 300hPa calculados usando todo el dominio o sólo un hemisferio por vez. 

```{r}
data[, hemisphere := ifelse(lon > 180, "W", "E")]

split <- data[, Regression2D(u ~ year | lon + lat, year)$field, by = hemisphere]

rbindlist(list(split = split[, -"hemisphere"], 
           full = cv_regression$field), idcol = "type", use.names = TRUE) %>% 

ggplot(aes(lon, lat)) +
   geom_contour_fill(aes(z = u),breaks = AnchorBreaks(0, exclude = 0)) +
   geom_world +
   scale_fill_divergent("Trend in V(1000hpa)") +
   coord_quickmap() +
   metR:::theme_field() +
   facet_wrap(~type, ncol = 1)
```

En los límites del dominio hay mucha diferencia y aún lejos hay sutiles cambios. Si hacemos un "zoom", el resultado de calcular la regresión en el dominio recortado o recordar la regresión global puede ser muy distinto:

```{r}
latlim <- c(30, 60)
lonlim <-  c(250, 300)
zoom <- data[lon %between% lonlim & lat %between% latlim] %>% 
   Regression2D(u ~ year | lon + lat, year, data = .)


rbindlist(list(zoom = zoom$field, 
           full = cv_regression$field[lon %between% lonlim & lat %between% latlim]),
          idcol = "type", use.names = TRUE) %>% 
ggplot(aes(lon, lat)) +
   geom_contour_fill(aes(z = ifelse(type == "zoom", u, NA)),
                     breaks = AnchorBreaks(0, exclude = 0)) +
   geom_contour_fill(aes(z = ifelse(type == "full", u, NA)),
                     breaks = AnchorBreaks(0, exclude = 0)) +
   # geom_raster(aes(fill = u)) +
   geom_world +
   scale_fill_divergent("Trend in V(1000hpa)") +
   coord_quickmap(ylim = latlim, xlim = lonlim) +
   metR:::theme_field() +
   facet_wrap(~type, ncol = 2)
```

La diferencia en la magnitud es enorme, pero también la distribución. El resultado del zoom no es estadísticamente significativo. 


# Bibliografía