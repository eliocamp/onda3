---
title: "Field Significance of Regression Patterns"
author: "Elio"
output: 
   pdf_document
urlcolor: blue
bibliography: "field-signf.bib"
---

```{r setup, include=FALSE}
# Notification
start.time <- unclass(Sys.time())
min.time <- 10
knit_doc <- knitr::knit_hooks$get("document")
knitr::knit_hooks$set(document = function(x) {
   took <- unclass(Sys.time()) - start.time
   if (unclass(Sys.time()) - start.time >= min.time) {
      notify("Done knitting!", 
             paste0("Took ", round(took), " seconds"),
             time = 5)
   }  
   knit_doc(x)
})

name <- tools::file_path_sans_ext(knitr::current_input())
knitr::opts_chunk$set(echo = FALSE,
                      cache = TRUE,
                      cache.extra = 11, 
                      warning = FALSE, message = FALSE,
                      out.extra = "", 
                      cache.path = paste0("cache/", name, "/"),
                      fig.path = paste0("fig/", name, "/"))

library(metR)
library(data.table)
library(ggplot2)
library(metR)
library(magrittr)
library(circular)
library(RcppRoll)
library(patchwork)
library(lubridate)

here <- here::here
source("scripts/helperfun.R")

data.world <- BuildMap(res = 1, smooth = 1)
map.world <- geom_map2(data.world)
map.SH <- geom_map2(data.world[lat %b% c(-90, 20)], color = "gray20")

lev.breaks <- c(1000, 500, 300, 200, 100, 50, 10)

theme_elio <- theme_minimal(base_size = 11) +
   theme(
      # text = element_text(family = font_rc),
      legend.position = "bottom", legend.box = "vertical",
      panel.spacing.y = unit(5, "mm"),
      panel.spacing.x = unit(5, "mm"),
      legend.spacing = unit(2, "mm"),
      plot.margin = grid::unit(rep(3, 4), "mm"),
      legend.title = element_blank(),
      legend.box.spacing = unit(3, "mm"),
      legend.margin = margin(t = -5),
      panel.grid = element_line(color = "gray50", size = 0.2, linetype = 3),
      panel.ontop = TRUE)
theme_set(theme_elio)
guide_colorstrip_bottom <- function(width = 25, height = 0.5, ...) {
   guide_colorstrip(title.position = "top", title.hjust = 0.5,
                    barheight = height,
                    barwidth = width, ...)
}
```

Esto es un intento de implementar y entender el método propuesto por [@DelSole2011] para calcular la significancia de los patrones de regresión. 

## Idea básica

La idea es que dado una serie de campos bidimensionales a lo largo del tiempo uno puede armar un campo de regresiones a partir de una serie temporal. Si el campo tiene M celdas, entonces hay que calcular M regresiones:


$$
y(t) = \beta_1x_1(t) + \epsilon_1(t) \\
y(t) = \beta_2x_2(t) + \epsilon_2(t) \\
\cdots  \\
y(t) = \beta_Mx_M(t) + \epsilon_M(t) \\
$$


Donde $x_i(t)$ representa el valor de la variable espacial en la celda iésima y el campo de regresión son los coeficientes $\beta_1, \beta_2, ...\beta_M$. Testear la significancia de este campo no es trivial. En general lo que se hace es testear cada $\beta_i$ por separado como si fuera independiente y obtener un "campo de significancia". Pero eso en realidad no es del todo válido porque ignora la correlación espacial entre los coeficientes y, más importante aún, la multiplicidad de tests. 

[@DelSole2011] propone cambiar el problema. En vez de hacer M regresiones simples, hacer una regresión múltiple: 

$$
y(t) = \beta_1x_1(t) + \beta_2x_2(t) + ... + \beta_Mx_M(t) + \epsilon(t)
$$

para la cual se puede testear la $H_0 := B_i = 0 \; \forall i= 1, 2,...M$ sin más dificultad. Esto elimina el problema de multiplicidad y la correlación espacial. Simple, ¿no? Listo..

No tan rápido. 

## Selecciónd el modelo

Esta regresión funciona si la cantidad de predictores ($M$) es menor que la cantidad de datos usados para realizar el ajuste ($N$), pero en la mayoría de los casos hay mas más celdas que datos. Por ejemplo, un campo global de 2.5° de resolución se tiene $144\times 72 = 10368$ celdas. Si queremos hacer un mapa de regresión con la ecuación anterior usando datos mensuales serían necesarios más de 864 años de datos. Imposible. 

El truquito ahora está en reconocer que la correlación espacial implica que en un campo global no hay $10368$ predictores independientes; existe mucha redundancia de información. Una forma de aprovechar esto es hacer la regresión en el espacio de las componentes principales usando sólo algunas (y siempre menos que $N$) y luego reconstruir el campo conseguido. Si la matriz de datos es $X$ (donde cada columna es un $x_i(t)$), hacemos 

$$
X = UDV^t
$$

Y pasamos a hacer la regresión

$$
y(t) = \beta_1u_1(t) + \beta_2u_2(t) + ... + \beta_Ku_K(t) + \epsilon(t)
$$

Donde $u_i$ son las columnas de $U$. Con $K < N$ nos aseguramos que la regresión ande bien. Luego, el campo de regresión es 
$$
BV^t
$$

(hay algunas constantes de normalización dando vuelta que no son demasiado importantes a la teoría --pero sí a la práctica!)

Simple, ¿no? Listo..

No tan rápido. 

¿Cuántas y cuáles componentes principales elegir? Es el problema eterno. El paper propone seleccionar las primeras K componentes principales usando validación cruzada para juzgar el "mejor K". El procedimiento es el siguiente:

Para k = 1 se ajusta el modelo N veces dejando de lado una observación por vez y computando la diferencia entre el y observado y el modelado. Se consiguen N errores y de estos se computa el $MSE$ y se asume que éste tiene un desvío estándar de $MSE/\sqrt{N}$. Se repíte el procedimiento para todos los k. El resutlado es una serie de $MSE(k)$. Se elije el mayor $k$ para el cual el $MSE$ esté dentro del intervalo de 1 desvío estándar del menor $MSE$ observado. 

De forma general, esto es un problema de *feature selection* que tiene otras soluciones. Una alternativa es usar [LASSO](https://en.wikipedia.org/wiki/Lasso_(statistics)) o [ridge regression](https://en.wikipedia.org/wiki/Tikhonov_regularization), que penaliza los coeficientes "grandes". 

## Ejemplos

### Viento zonal 

Voy a usar datos de viento zonal medio de DEF en 300hPa en el hemisferio norte para replicar lo que hicieron los autores del paper. 

```{r}
library(ggplot2)
library(metR)
library(data.table)
library(magrittr)

world <- subset(map_data("world2"), lat > 0)

geom_world <- geom_path(data = world, aes(long, lat, group = group),
                        size = 0.2)

data <- ReadNetCDF("~/DATOS/NCEP Reanalysis/uwnd.mon.mean.nc", c(var = "uwnd"),
                   subset = list(level = 300,
                                 lat = c(0:90),
                                 time = c("1948-01-01", "2009-12-31"))) %>% 
   .[month(time) %in% c(12, 1, 2)] %>% 
   .[, .(var = mean(var)), by = .(lat, lon, year(time))] 
```

```{r}
source("scripts/Regression2D_bk.R")
```

> Una aclaración importante es que los autores del paper usan leave-one-out crossvalidation, pero eso es **eterno** para una cantidad de datos medianamente grande, así que yo implementé k-fold crossvalidation con k = 10 por default. 

```{r}
all_regression <- function(data) {
   
   point<- data[, FitLm(var, year, se = TRUE), by = .(lon, lat)]
   cv <- Regression2D(var ~ year | lat + lon, year, data = data) 
   lasso <- Regression2D(var ~ year | lat + lon, year, data = data,
                         method = "lasso") 
   g <- rbindlist(list(point = point[term == "year",
                                     .(lon, lat, var = estimate)],
                       cv = cv$field, 
                       lasso = lasso$field), 
                  use.names = TRUE, idcol = "type") %>% 
      ggplot(aes(lon, lat)) +
      # geom_raster(aes(fill = u)) +
      geom_contour_fill(aes(z = var),breaks = AnchorBreaks(0, exclude = 0)) +
      geom_world +
      scale_fill_divergent("Trend in U(300hpa)") +
      coord_quickmap() +
      metR:::theme_field() +
      facet_wrap(~type, ncol = 2)
   
   list(point = point, cv = cv, lasso = lasso, plot = g)
}

regression_u <- all_regression(data)
```


```{r, fig.height=5}
regression_u$plot
```

Los campos de regresión son prácticamente iguales. La naturaleza de LASSO, que penaliza coeficientes grandes, hace que los valores absolutos sean menores. Podemos ver cuándos EOFs se usaron en la regresión, el $r^2$ del fit y su p-valor siguiendo la metodología del paper

```{r}
rbind(cv = regression_u$cv$summary,
      lasso = regression_u$lasso$summary) %>% 
   knitr::kable(digits = c(2, 2, 16, 0))
```

Aunque lo patrones son similares, usan distinta cantidad de EOF. La estimación del p-valor no sé si es válida para LASSO, ya que no es cuadrados mínimos, así que la distribución del $r^2$ posiblemente no sea la que dicen en el paper. Pero para ser justos, tampoco sé si con la validación cruzada sigue valiendo. 

Un detalle para el caso de LASSO es que los EOFs que usa varían muchísimo si cambio la cantidad de EOFs que permito entrar a la regresión desde un principio

```{r}
table <- as.data.table(expand.grid(max_eof = c(round(seq(3, 60, length.out = 7)), 61:63),
                                   type = c("lasso", "cv", "neof"))) %>% 
   .[, Regression2D(var ~ year | lat + lon, year, data = data,
                    max_eof = max_eof,
                    method = type)$summary,
     by = .(type, max_eof)] 

lasso_v_cv <- function(value, data = table) {
   dcast(data[type != "neof"], max_eof ~ type, value.var = value)
}

lasso_v_cv(c("non_zero", "r2"))[, lapply(.SD, round, 2)] %>% 
   knitr::kable()
```

En particular, se ve que la cantidad de coeficientes no nulos aumenta a medida que se ponen más eofs pero luego disminuye mucho cuando se ponen cerca del máximo. La crossvalidación, en cambio, se queda estable y no es sensible a ese problema (al menos para este ejemplo!)

El $r^2$ también tiene un comportamietno similar. Comparando el $r^2$ en función de la cantidad de coeficientes no nulos para el método LASSO, crossvalidación y el "ingénuo" se ve que el método de crossvalidación deja de incluir EOFs en el "codo" de la curva, mientras que LASSO los sigue incluyendo. 

```{r}
ggplot(table, aes(non_zero, r2)) + 
   geom_point() +
   facet_wrap(~type, ncol = 1)
```

¿Por qué luego baja cuando se incluyen todos los EOFs? No sé. Es posible que tenga que ver con el algoritmo de LASSO, que se comporta diferente según la cantidad de variables en la regresión. Misterio. 

### Viento meridional en  1000hPa

Para probar con otros datos.

```{r}
datav <- ReadNetCDF("~/DATOS/NCEP Reanalysis/vwnd.mon.mean.nc", c(var = "vwnd"),
                    subset = list(level = 1000,
                                  lat = c(0:90),
                                  time = c("1948-01-01", "2009-12-31"))) %>% 
   .[month(time) %in% c(12, 1, 2)] %>% 
   .[, .(var = mean(var)), by = .(lat, lon, year(time))] 
```


```{r}
regression_v <- all_regression(datav)
```

```{r}
regression_v$plot
```

En estos datos, ambos métodos dan buenos resultados caracterizando el campo de regresión. LASSO tiene los mismos problemas que en el caso anterior. 

## Problemas

### Dependencia con dominio

Un problema de los EOFs es que pueden depender mucho del dominio. Por lo tanto, es esperable que los mapas de regresión también dependan del dominio si se usan EOFs para generarlos. Por ejemplo, estos son los mapas de regresión para U en 300hPa calculados usando todo el dominio o sólo un hemisferio por vez. 

```{r}
data[, WE := ifelse(lon > 180, "W", "E")][, NS := ifelse(lat > 45, "N", "S")]

split <- data[, Regression2D(var ~ year | lon + lat, year)$field, 
              by = .(quadrant = interaction(WE, NS))]

rbindlist(list(split = split[, -"quadrant"], 
               full = regression_u$cv$field), idcol = "type", use.names = TRUE) %>% 
   ggplot(aes(lon, lat)) +
   geom_contour_fill(aes(z = var),breaks = AnchorBreaks(0, exclude = 0)) +
   geom_world +
   geom_hline(yintercept = 45, size = 0.5, linetype = 3) +
   geom_vline(xintercept = 180, size = 0.5, linetype = 3) +
   scale_fill_divergent("Trend in V(1000hpa)") +
   coord_quickmap() +
   metR:::theme_field() +
   facet_wrap(~type, ncol = 1)
```

En algunas partes hay diferencias, aunque en otras no tanto. El cuadrante noroeste perdió toda su magnitud. Si hacemos un "zoom", el resultado de calcular la regresión en el dominio recortado o recordar la regresión global puede ser muy distinto:

```{r}
latlim <- c(30, 60)
lonlim <-  c(250, 300)
zoom <- data[lon %between% lonlim & lat %between% latlim] %>% 
   Regression2D(var ~ year | lon + lat, year, data = .)


rbindlist(list(zoom = zoom$field, 
               full = regression_u$cv$field[lon %between% lonlim & lat %between% latlim]),
          idcol = "type", use.names = TRUE) %>% 
   ggplot(aes(lon, lat)) +
   geom_contour_fill(aes(z = ifelse(type == "zoom", var, NA)),
                     breaks = AnchorBreaks(0, exclude = 0)) +
   geom_contour_fill(aes(z = ifelse(type == "full", var, NA)),
                     breaks = AnchorBreaks(0, exclude = 0)) +
   # geom_raster(aes(fill = u)) +
   geom_world +
   scale_fill_divergent("Trend in V(1000hpa)") +
   coord_quickmap(ylim = latlim, xlim = lonlim) +
   metR:::theme_field() +
   facet_wrap(~type, ncol = 2)
```

La diferencia en la magnitud es enorme, pero también la distribución. El resultado del zoom no es estadísticamente significativo. 

### Dependencia con la longitud de la serie

```{r}
data_subset <- data[year %in% JumpBy(unique(year), 3)]
```

¿Qué pasa si saco datos? Digamos que tengo un tercio de los datos (`r data_subset[, uniqueN(year)]` años).

```{r}
short_regression_u <- all_regression(data_subset)
```


```{r}
short_regression_u$plot
```

Las estimaciones cambian, obviamente, pero se ve que LASSO prácticamente no encuentra regressiones importantes. Hay un solo EOF con coeficiente no nulo para LASSO, pero el p.valor calculado por la fórmula estándar da `r round(short_regression_u$lasso$summary$p.value, 4)`. O sea, bastante significativo. El p.valor de crosscorrelación es `r round(short_regression_u$cv$summary$p.value, 4)`; no muy significativo. 


### Dependencia con la resolución

¿Depende de la resolución de los datos? Voy a usar datos de ERA Interim con 2.5x2.5 para la baja resolución y con 0.75x0.75 para la alta resolución. 

```{r}
data <- rbindlist(
   list(lowres = ReadNetCDF("DATA/monthly_u_2.5.nc", c(var = "u"),
                            subset = list(latitude = c(0:90))),
        higres = ReadNetCDF("DATA/monthly_u.nc", c(var = "u"),
                            subset = list(latitude = c(0:90)))), idcol = "res") %>% 
   setnames(c("longitude", "latitude"), c("lon", "lat")) %>% 
   .[month(time) %in% c(12, 1, 2)] %>% 
   .[, .(var = mean(var)), by = .(lat, lon, year(time), res)] 
```

```{r}
lohi_regr <- lapply(unique(data$res), function(r) {
   data[lat %between% latlim & lon %between% lonlim & res == r] %>% 
      Regression2D(var ~ year | lon + lat, year, method = "cv",  data = .)
})
names(lohi_regr) <- unique(data$res)

point <- data[res == "higres", FitLm(var, year), by = .(lon, lat)][term == "year"] %>% 
   .[, .(lon, lat, var = estimate)]
```

```{r}
library(patchwork)
list(lowres = lohi_regr$lowres$field,
     hires = lohi_regr$higres$field
     # point = point
     ) %>% 
   rbindlist(idcol = "res") %>% 
   ggplot(aes(lon, lat)) +
   # geom_raster(aes(fill = u)) +
   geom_contour_fill(aes(z = var),breaks = AnchorBreaks(0, exclude = 0)) +
   geom_world +
   scale_fill_divergent("Trend in U(300hpa)") +
   coord_quickmap(ylim = latlim, xlim = lonlim) +
   metR:::theme_field() +
   facet_wrap(~res, ncol = 2) +
   
   ggplot(point, aes(lon, lat)) +
   # geom_raster(aes(fill = u)) +
   geom_contour_fill(aes(z = var),breaks = AnchorBreaks(0, exclude = 0)) +
   geom_world +
   scale_fill_divergent("Trend in U(300hpa)") +
   coord_quickmap(ylim = latlim, xlim = lonlim) +
   metR:::theme_field() +
   plot_layout(ncol = 1)
```

Las distintas resoluciones dan algo ligeramente distinto, pero no mucho. El tema es que analizando los coeficientes, sólo la primera componente principal da no nula y no se parece en nada a la tendencia punto a punto. Además notar la enorme diferencia en la magnitud del efecto. Lo que está pasando es que el ajuste no es significativo (pvalor = 1 para ambas resoluciones). Si pasamos a otra región, la cosa es distinta:

```{r}
latlim <- c(20, 50) 
lonlim <- c(175, 225)

lohi_regr <- lapply(unique(data$res), function(r) {
   data[lat %between% latlim & lon %between% lonlim & res == r] %>% 
      Regression2D(var ~ year | lon + lat, year, method = "cv",  data = .)
})
names(lohi_regr) <- unique(data$res)

point <- data[res == "higres", FitLm(var, year), by = .(lon, lat)][term == "year"] %>% 
   .[, .(lon, lat, var = estimate)]

library(patchwork)
list(lowres = lohi_regr$lowres$field,
     hires = lohi_regr$higres$field
     # point = point
     ) %>% 
   rbindlist(idcol = "res") %>% 
   ggplot(aes(lon, lat)) +
   # geom_raster(aes(fill = u)) +
   geom_contour_fill(aes(z = var),breaks = AnchorBreaks(0, exclude = 0)) +
   geom_world +
   scale_fill_divergent("Trend in U(300hpa)") +
   coord_quickmap(ylim = latlim, xlim = lonlim) +
   metR:::theme_field() +
   facet_wrap(~res, ncol = 2) +
   
   ggplot(point, aes(lon, lat)) +
   # geom_raster(aes(fill = u)) +
   geom_contour_fill(aes(z = var),breaks = AnchorBreaks(0, exclude = 0)) +
   geom_world +
   scale_fill_divergent("Trend in U(300hpa)") +
   coord_quickmap(ylim = latlim, xlim = lonlim) +
   metR:::theme_field() +
   plot_layout(ncol = 1)

```

En esta región del pacífico donde las tendencias son más intensas, el recorte no cambia mucho el patrón de regresión. La regresion es estadísticamente significativa en ambas resoluciones. La intensidad del cambio es similar. 


# Conclusiones

1. El método LASSO para reducir la dimensionalidad del problema no da buenos resultados. El p-valor obtenido es totalmente inválido y la candiad de coeficientes no nulos es muy sensible a la cantiad de compoentes principales que se permite entrar en la regresión. Además suele dar valores subestimados en la regresión (aunque eso es por diseño).

1. Hay algunos problemas con la elección del dominio que cambian el valor de la regresión. Sin embargo, estos cambios son informativos, ya que la variación es grande donde la señal es pequeña. 

1. El p.valor conseguido es informativo!

1. Tiene una gran limitación: no acepta valores faltantes! Se puede usar DINEOF para rellenar usando EOF y tener algo consistente.

# Bibliografía